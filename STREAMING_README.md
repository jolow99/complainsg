# Word-by-Word Streaming Implementation

This implementation provides real-time word-by-word streaming of LLM responses through WebSocket connections.

## How It Works

### Backend (server.py)

1. **WebSocket Connection**: The server accepts WebSocket connections at `/ws`
2. **Message Processing**: When a message is received, it:
   - Updates the shared store with the user's message
   - Runs the flow in a separate thread
   - Monitors the `llm_output` in the shared store
3. **Word-by-Word Streaming**: 
   - Compares current output with previous output
   - Extracts new words that were added
   - Sends each new word as a separate `stream_chunk` message
4. **Interrupt Support**: Handles interrupt signals to stop ongoing streams

### Frontend (chat.tsx)

1. **WebSocket Connection**: Connects to the backend WebSocket
2. **Message Handling**: 
   - Receives `stream_chunk` messages
   - Appends each word to the current assistant message
   - Updates the UI in real-time
3. **Interrupt Button**: Shows a "Stop Generation" button during streaming

## Message Types

### From Client to Server
- `{"content": "user message"}` - Regular user message
- `{"type": "interrupt"}` - Interrupt signal

### From Server to Client
- `{"type": "connection", "message": "..."}` - Welcome message
- `{"type": "message_received", "content": "..."}` - Message acknowledgment
- `{"type": "stream_chunk", "content": "word"}` - Individual word from stream
- `{"type": "stream_complete", "content": ""}` - Stream completion signal
- `{"type": "interrupt_acknowledged", "message": "..."}` - Interrupt acknowledgment
- `{"type": "error", "message": "..."}` - Error message

## Key Features

1. **Real-time Streaming**: Words appear as they're generated by the LLM
2. **Interrupt Capability**: Users can stop generation at any time
3. **Thread Safety**: Uses separate threads for flow execution and WebSocket monitoring
4. **Error Handling**: Comprehensive error handling for WebSocket and JSON parsing
5. **Clean UI**: Smooth word-by-word updates in the chat interface

## Testing

Run the test script to verify the implementation:

```bash
python test_streaming.py
```

This will connect to the WebSocket server and demonstrate the word-by-word streaming functionality.

## Architecture

```
User Input → WebSocket → Server → Flow Execution → LLM Stream → Word Extraction → WebSocket → Frontend → UI Update
```

The implementation ensures that each word is sent individually as soon as it becomes available from the LLM stream, providing a smooth real-time experience. 